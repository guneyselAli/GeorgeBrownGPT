{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Availability(full_scraped_data):\n",
    "    # Split the full scraped data into lines\n",
    "    lines = full_scraped_data.strip().split('\\n')\n",
    "\n",
    "    # Find indices where the specific headers are present\n",
    "    header_indices = [i for i, line in enumerate(lines) if line in [\"Program Availability\", \"Semester\", \"Domestic\", \"International\"]]\n",
    "\n",
    "    # Initialize json_output with a default value\n",
    "    json_output = None\n",
    "    \n",
    "    # Iterate over each found header index\n",
    "    for start_index in header_indices[::4]:\n",
    "        # Check if all subsequent headers are present\n",
    "        if all(header in lines[start_index:start_index + 4] for header in [\"Semester\", \"Domestic\", \"International\"]):\n",
    "            # Extract the next 16 lines starting from the found index\n",
    "            chunk_data = lines[start_index:start_index + 16]\n",
    "\n",
    "            # Extract headers and data\n",
    "            headers = chunk_data[:4]\n",
    "            data = [chunk_data[j:j + 4] for j in range(4, len(chunk_data), 4)]\n",
    "\n",
    "            # Create a list to store the sentences\n",
    "            sentences = []\n",
    "\n",
    "            # Iterate over the data and create sentences\n",
    "            for k in range(len(data)):\n",
    "                semester_year = f\"{data[k][0]} {data[k][1]}\"\n",
    "                domestic_availability = f\"{semester_year} Domestic {data[k][2].lower()}\"\n",
    "                international_availability = f\"{semester_year} International {data[k][3].lower()}\"\n",
    "                \n",
    "                sentences.extend([domestic_availability, international_availability])\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            columns = ['Semester', 'Year', 'Intake', 'Availability']\n",
    "            data_for_df = [sentence.split()[:3] + [' '.join(sentence.split()[3:])] for sentence in sentences]\n",
    "            df = pd.DataFrame(data_for_df, columns=columns)\n",
    "\n",
    "            # Convert DataFrame to JSON\n",
    "            json_output = json.loads(df.to_json(orient='records'))\n",
    "\n",
    "    return json_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_info1(scraped_data):\n",
    "    # Define a regex pattern to match key-value pairs\n",
    "    pattern = re.compile(r'([^:\\n]+):\\s*([^\\n]+)')\n",
    "\n",
    "    # Search for the Program Name to Program Availability subset\n",
    "    subset_match = re.search(r'Program Name:.*Program Availability', scraped_data, re.DOTALL)\n",
    "\n",
    "    if subset_match:\n",
    "        # Find matches in the subset\n",
    "        matches = pattern.findall(subset_match.group())\n",
    "\n",
    "        # Create a dictionary from the matches\n",
    "        metadata_dict = dict(matches)\n",
    "        \n",
    "        return metadata_dict\n",
    "    else:\n",
    "        return None  # Return None if the subset is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_contact_chunk(full_scraped_data):\n",
    "    # Find the starting index of the \"Contact Us\" section\n",
    "    start_index1 = full_scraped_data.find(\".\\nContact Us\")\n",
    "    start_index2 = full_scraped_data.find('English assessment\\nContact Us')\n",
    "    start_index3 = full_scraped_data.find('Contact Us\\nContact')\n",
    "\n",
    "    # Choose the minimum valid start index\n",
    "    valid_start_indices = [index for index in [start_index1, start_index2, start_index3] if index != -1]\n",
    "    if not valid_start_indices:\n",
    "        return None  # Return None if no valid start index is found\n",
    "\n",
    "    start_index = min(valid_start_indices)\n",
    "\n",
    "    # Find the ending index of the \"Visit\" section\n",
    "    end_index1 = full_scraped_data.find(\"Visit\", start_index)\n",
    "    end_index2 = full_scraped_data.find(\"For more information about George Brown College\", start_index)\n",
    "\n",
    "    # Choose the minimum valid end index\n",
    "    valid_end_indices = [index for index in [end_index1, end_index2] if index != -1]\n",
    "    if not valid_end_indices:\n",
    "        return None  # Return None if no valid end index is found\n",
    "\n",
    "    end_index = min(valid_end_indices)\n",
    "\n",
    "    # Extract the chunk between start and end indices\n",
    "    contact_chunk = full_scraped_data[start_index:end_index].strip()\n",
    "\n",
    "    # Remove the first line if it doesn't contain \"English assessment\" or \".\"\n",
    "    lines = contact_chunk.split('\\n')\n",
    "    if lines and not any(keyword in lines[0] for keyword in ['Contact Us']):\n",
    "        lines = lines[1:]\n",
    "\n",
    "    return '\\n'.join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_contact(base_url1,chunk):\n",
    "    prompt_temp1 = \"\"\"\"\n",
    "    You are a helpful AI model who can create JSON from text. Your task is to extract all key information from the given text and present it as in JSON (structured format). Output should only be JSON object no other text required. Here are some examples:\n",
    "\n",
    "    Example 1:\n",
    "    Input:\n",
    "    Contact Us\n",
    "    School of Deaf and Deafblind Studies\n",
    "    Email:\n",
    "    communityservices@georgebrown.ca\n",
    "    Our office hours are 8 a.m. – 4 p.m.\n",
    "    Erika Stebbings, ASL & Deaf Studies Program Co-ordinator\n",
    "    Email:\n",
    "    erika.stebbings@georgebrown.ca\n",
    "\n",
    "    Output:\n",
    "    {\n",
    "    \"Contact\": \"School of Deaf and Deafblind Studies\",\n",
    "    \"Contact email\": \"communityservices@georgebrown.ca\",\n",
    "    \"Office Hours\": \"8 a.m. – 4 p.m\",\n",
    "    \"Program Co-ordinator\": \"Erika Stebbings\",\n",
    "    \"Co-ordinator email\": \"erika.stebbings@georgebrown.ca\"\n",
    "    }\n",
    "\n",
    "    Example 2:\n",
    "    Input:\n",
    "    Contact Us\n",
    "    School of Computer Technology\n",
    "    Phone: 416-415-5000, ext. 4287\n",
    "    Email:\n",
    "    computertechnology@georgebrown.ca\n",
    "    The office hours are:\n",
    "    Monday – Friday: 9 a.m. – 4 p.m.\n",
    "    Program Co-ordinator: Moe Fadaee\n",
    "    Email:\n",
    "    Moe.Fadaee@georgebrown.ca\n",
    "    Phone: 416-415-5000, ext. 3229\n",
    "\n",
    "    Output:\n",
    "    {\n",
    "    \"Contact\": \"School of Computer Technology\",\n",
    "    \"Contact email\": \"computertechnology@georgebrown.ca\",\n",
    "    \"Phone\": \"416-415-5000, ext. 4287\",\n",
    "    \"Office Hours\": \"Monday – Friday: 9 a.m. – 4 p.m.\",\n",
    "    \"Program Co-ordinator\": \"Moe Fadaee\",\n",
    "    \"Co-ordinator email\": \"Moe.Fadaee@georgebrown.ca\",\n",
    "    \"Co-ordinator Phone\": \"416-415-5000, ext. 3229\"\n",
    "    }\n",
    "\n",
    "    Now, extract the key information from the following text:\"\"\"\n",
    "    llm1 =  Ollama(\n",
    "    model=\"mistral\",\n",
    "    base_url=base_url1\n",
    "    )\n",
    "\n",
    "    parser = JsonOutputParser()\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"{prompt_temp}.\\n{query}\\n\\n {format_instructions}\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt | llm1 | parser\n",
    "\n",
    "    contact_info = chain.invoke({ \"prompt_temp\":prompt_temp1 ,\"query\": chunk })\n",
    "    return contact_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information from the scraped data\n",
    "def extract_info(data,base_url1):\n",
    "    program_info = {}\n",
    "    \n",
    "    # Extract Program Information\n",
    "    program_info[\"Program Information\"] = program_info1(data)\n",
    "    \n",
    "    \n",
    "    # Extract Program Availability\n",
    "    program_info['Availability'] =Availability(data)\n",
    "    \n",
    "    contact_chunk = extract_contact_chunk(data)\n",
    "    print(contact_chunk)\n",
    "    program_info['Contact Related Information'] = extract_contact(base_url1,contact_chunk)\n",
    "\n",
    "    return program_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(file_path, metadata_folder,base_url1):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        full_scraped_data = file.read()\n",
    "\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]  # Get the filename without extension\n",
    "    metadata_file_path = os.path.join(metadata_folder, f\"{file_name}.json\")\n",
    "    \n",
    "    # Extract information\n",
    "    extracted_info = extract_info(full_scraped_data,base_url1)\n",
    "\n",
    "\n",
    "    # Save extracted information to JSON file in metadata folder\n",
    "    with open(metadata_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(extracted_info, json_file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url1 = \"https://e590-35-247-83-61.ngrok-free.app\"\n",
    "folder_path = \"test1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied A.I. Solutions Development Program Postgraduate T431  George Brown College2023\n",
      "****************************************************************************************\n",
      "Architectural Technician Program T132  George Brown College2023\n",
      "****************************************************************************************\n",
      "Architectural Technology Program T109  George Brown College2023\n",
      "****************************************************************************************\n",
      "Art and Design Foundation Program G108  George Brown College2023\n",
      "****************************************************************************************\n",
      "Autism and Behavioural Science Program Postgraduate C405  George Brown College2023\n",
      "****************************************************************************************\n",
      "Automation Technician Program Distance Education T950  George Brown College2023\n",
      "****************************************************************************************\n",
      "Bachelor of Business Administration Trades Management Honours Pathway Building Renovation Technology Program B304  George Brown College2023\n",
      "****************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"test1\"\n",
    "    metadata_folder = \"metadata_\"+folder_path\n",
    "\n",
    "    # Create metadata folder if it doesn't exist\n",
    "    os.makedirs(metadata_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if the path is a file and has a .txt extension\n",
    "        if os.path.isfile(file_path) and filename.lower().endswith('.txt'):\n",
    "            print(filename.replace('.txt', ''))\n",
    "            process_text_file(file_path, metadata_folder,base_url1)\n",
    "            print('****************************************************************************************')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
